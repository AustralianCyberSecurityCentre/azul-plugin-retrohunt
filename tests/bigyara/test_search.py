import logging
import os
import sys
import tempfile
import unittest
from hashlib import sha256

from azul_plugin_retrohunt import test_utils
from azul_plugin_retrohunt.bigyara.index import BigYaraIndexer
from azul_plugin_retrohunt.bigyara.ingest import BigYaraIngestor
from azul_plugin_retrohunt.bigyara.search import (
    DataCallbackException,
    NoAtomException,
    QueryTypeEnum,
    RuleFileMatches,
    SearchPhaseEnum,
    search,
)
from azul_plugin_retrohunt.bigyara.yara_parse import YaraStringNoAtomException
from azul_plugin_retrohunt.models import FileMetadata

# FUTURE: bigyara search should in theory be usable on any biggrep indices,
#               not just ones generated by the bigyara indexer.
#               all tests that use the bigyara indexer api should be
#               switched to just use a call to the biggrep indexer, since there
#               shouldn't be a dependency.

logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s %(name)s [%(levelname)s] %(message)s",
    datefmt="%Y%m%d %H:%M:%s",
    stream=sys.stderr,
)

content_dict: dict[str, bytes] = {}


def to_hash_dict(content_list: list[bytes]) -> dict[str, bytes]:
    """Convert a list of data into a dictionary keyed by the hash of the data."""
    global content_dict
    content_dict = {}
    for content in content_list:
        content_dict[sha256(content).hexdigest()] = content
    return content_dict


def fetch_from_dict(path: str, unused: dict) -> bytes:
    """Progress callback to get the data from the dict by extracting it's hash."""
    hash = path.split("/")[-1]
    if hash in content_dict:
        return content_dict[hash]
    else:
        return None


# FUTURE: more search tests:
#               - multiple index locations
#               - identical index deduplication
#               - variations in broad and narrow results - all, some and none
#               - complex yara regex
#               - all yara modifiers
#               - complex suricata rule
#               exceptions:
#               - invalid index (bgparse error)
#               - exception in progress callback
#               - yara atom parse error (eg. repeated string name)
#               - yara matching error
#               - suricata error


class TestSearch(test_utils.BaseIngestorIndexerTest):

    def setUp(self):
        self.string_content: list[bytes] = [b"abcdefgh", b"ABCDRSTVFGH", b"abcdabcd"]
        return super().setUp()

    def index_string_content(self):
        """Index some data to use in search tests."""
        for content in self.string_content:
            self.ingestor.add_data_to_index_cache(
                content, FileMetadata(stream_label="content", stream_source="testing")
            )
        self.indexer.generate_index(self.ingestor.cache_directory)

    def index_data(self, content_list: list[bytes]):
        """Index some data to use in search tests."""
        for content in content_list:
            self.ingestor.add_data_to_index_cache(
                content, FileMetadata(stream_label="content", stream_source="testing")
            )
        self.indexer.generate_index(self.ingestor.cache_directory)

    def test_short_query(self):
        """Should fail since the query is too short."""
        self.index_string_content()
        self.assertRaises(NoAtomException, search, "a", QueryTypeEnum.STRING, self.base_temp_dir)

    def test_no_data_fetch(self):
        """Test that failures in fetching the data are handled correctly."""

        def no_data(path: str, config: dict) -> bytes:
            return None

        def error_data(path: str, config: dict) -> bytes:
            raise Exception("There is no data!")

        yara_rule = """
        rule Rule
        {
            strings:
                $a = "abcd"
            condition:
                any of them
        }
        """
        self.index_string_content()

        results: RuleFileMatches

        # all broad phase matches should be skipped since data cannot be retrieved
        results = search(yara_rule, QueryTypeEnum.YARA, self.base_temp_dir, no_data)
        self.assertEqual(results, {})

        with self.assertRaises(DataCallbackException):
            results = search(yara_rule, QueryTypeEnum.YARA, self.base_temp_dir, error_data)

        with self.assertRaises(ValueError):
            results = search(yara_rule, QueryTypeEnum.YARA, self.base_temp_dir, None)

    def test_progress_callback(self):
        """Test that the progress callback gets passed the correct data."""
        global content_dict
        content_dict = to_hash_dict(self.string_content)

        def progress_callback(phase: int, done: int, total: int, new_match: tuple[str, list[str | bytes]]):
            nonlocal self
            self.assertTrue(
                phase == SearchPhaseEnum.ATOM_PARSE
                or phase == SearchPhaseEnum.BROAD_PHASE
                or phase == SearchPhaseEnum.NARROW_PHASE
            )
            self.assertLessEqual(done, total)

            if phase == SearchPhaseEnum.ATOM_PARSE:
                self.assertEqual(total, 1)
                if done == 1:
                    self.assertEqual(new_match, ("Rule", [b"abcd", b"notfound"]))
                elif new_match is not None:
                    self.assertEqual(len(new_match[1]), 0)
            elif phase == SearchPhaseEnum.BROAD_PHASE:
                self.assertEqual(total, 2)
                if done == 1:
                    self.assertEqual(
                        new_match,
                        (
                            "Rule",
                            [
                                os.path.join(
                                    self.base_temp_dir,
                                    "content/cache/9c56cc51b374c3ba189210d5b6d4bf57790d351c96c47c02190ecf1e430635ab",
                                ),
                                os.path.join(
                                    self.base_temp_dir,
                                    "content/cache/3bc49b73e2fb201924d9dcce5fb6d6fd7cfbf58c49be8cc46439c05dc634b151",
                                ),
                            ],
                        ),
                    )
                elif new_match is not None:
                    self.assertEqual(len(new_match[1]), 0)
            elif phase == SearchPhaseEnum.NARROW_PHASE:
                self.assertEqual(total, 2)
                if done == 1:
                    self.assertEqual(
                        new_match,
                        (
                            "Rule",
                            [
                                os.path.join(
                                    self.base_temp_dir,
                                    "content/cache/9c56cc51b374c3ba189210d5b6d4bf57790d351c96c47c02190ecf1e430635ab",
                                ),
                            ],
                        ),
                    )
                elif new_match is not None:
                    self.assertEqual(len(new_match[1]), 0)

        self.index_string_content()

        yara_rule = """
        rule Rule
        {
            strings:
                $a = "abcd"
                $b = "notfound"
            condition:
                #a == 1 or $b
        }
        """

        results: RuleFileMatches = search(
            yara_rule, QueryTypeEnum.YARA, self.base_temp_dir, fetch_from_dict, progress_callback
        )

        self.assertDictEqual(
            results,
            {
                "Rule": [
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/9c56cc51b374c3ba189210d5b6d4bf57790d351c96c47c02190ecf1e430635ab",
                    )
                ]
            },
        )

    def test_string_search(self):
        """Test that a string search succeeds."""
        self.index_string_content()

        results: RuleFileMatches = search("abcd", QueryTypeEnum.STRING, self.base_temp_dir)
        self.assertDictEqual(
            results,
            {
                "abcd": [
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/9c56cc51b374c3ba189210d5b6d4bf57790d351c96c47c02190ecf1e430635ab",
                    ),
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/3bc49b73e2fb201924d9dcce5fb6d6fd7cfbf58c49be8cc46439c05dc634b151",
                    ),
                ]
            },
        )

    def test_yara_search(self):
        """Test that a yara search succeeds."""
        yara_rules = """
        rule Rule1
        {
            strings:
                $a = "abcd"
                $b = /ab*cd(blah)+efgh/
            condition:
                any of them
        }

        rule Rule2
        {
            strings:
                $a = "RSTV" nocase
                // FUTURE: test nocase regex with no valid atoms in first 6 chars
            condition:
                all of them
        }
        """

        global_total_complete: int = 0
        global_total: int = 0

        def callback_val(searchphase: int, done: int, total: int, ruleMatchTuple):
            nonlocal global_total_complete
            nonlocal global_total
            global_total_complete = done
            global_total = total

        global content_dict
        content_dict = to_hash_dict(self.string_content)

        self.index_data(list(content_dict.values()))

        results: RuleFileMatches = search(
            yara_rules, QueryTypeEnum.YARA, self.base_temp_dir, fetch_from_dict, callback_val
        )
        self.assertDictEqual(
            results,
            {
                "Rule1": [
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/9c56cc51b374c3ba189210d5b6d4bf57790d351c96c47c02190ecf1e430635ab",
                    ),
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/3bc49b73e2fb201924d9dcce5fb6d6fd7cfbf58c49be8cc46439c05dc634b151",
                    ),
                ],
                "Rule2": [
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/55293160622d3a69153282d9a2a1887342944ee8a03907462023d6b97b258184",
                    )
                ],
            },
        )

        self.assertEqual(global_total_complete, 3)
        self.assertEqual(global_total, 3)

        yara_rules = """
        rule Rule
        {
            strings:
                $a = "ab"
            condition:
                any of them
        }
        """
        # no atoms long enough
        with self.assertRaises(YaraStringNoAtomException):
            results: RuleFileMatches = search(yara_rules, QueryTypeEnum.YARA, self.base_temp_dir, fetch_from_dict)

    def test_yara_search_skipping_one_file(self):
        """Test that a yara search succeeds but also skip one file and ensure the progress callback works."""
        yara_rules = """
        rule Rule1
        {
            strings:
                $a = "abcd"
                $b = /ab*cd(blah)+efgh/
            condition:
                any of them
        }

        rule Rule2
        {
            strings:
                $a = "RSTV" nocase
                // FUTURE: test nocase regex with no valid atoms in first 6 chars
            condition:
                all of them
        }
        """

        global_total_complete: int = 0
        global_total: int = 0

        def callback_val(searchphase: int, done: int, total: int, ruleMatchTuple):
            nonlocal global_total_complete
            nonlocal global_total
            global_total_complete = done
            global_total = total

        global content_dict
        content_dict = to_hash_dict(self.string_content)

        self.index_data(list(content_dict.values()))

        def proxy_fetch_from_dict(path: str, unused: dict):
            if "9c56cc51b374c3ba189210d5b6d4bf57790d351c96c47c02190ecf1e430635ab" in path:
                return None
            return fetch_from_dict(path, unused)

        results: RuleFileMatches = search(
            yara_rules, QueryTypeEnum.YARA, self.base_temp_dir, proxy_fetch_from_dict, callback_val
        )
        self.assertDictEqual(
            results,
            {
                "Rule1": [
                    # Respect comments
                    # os.path.join(
                    #     self.base_temp_dir,
                    #     "content/cache/9c56cc51b374c3ba189210d5b6d4bf57790d351c96c47c02190ecf1e430635ab",
                    # ),
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/3bc49b73e2fb201924d9dcce5fb6d6fd7cfbf58c49be8cc46439c05dc634b151",
                    ),
                ],
                "Rule2": [
                    os.path.join(
                        self.base_temp_dir,
                        "content/cache/55293160622d3a69153282d9a2a1887342944ee8a03907462023d6b97b258184",
                    )
                ],
            },
        )

        self.assertEqual(global_total_complete, 2)
        self.assertEqual(global_total, 2)

    def test_suricata_search(self):
        """Test that a snort search succeeds."""
        # FUTURE suricata - implement
        # global content_dict

        # # using the snort rule from RE training
        # with open(os.path.join(os.path.dirname(__file__), "data/custom.rules"), "rb") as rules_file:
        #     snort_query: str = rules_file.read().decode()

        # with open(os.path.join(os.path.dirname(__file__), "data/simplebeacon1.pcap"), "rb") as pcap_file:
        #     data: bytes = pcap_file.read()
        #     content_dict[sha256(data).hexdigest()] = data
        # with open(os.path.join(os.path.dirname(__file__), "data/simplebeacon2.pcap"), "rb") as pcap_file:
        #     data: bytes = pcap_file.read()
        #     content_dict[sha256(data).hexdigest()] = data

        # for content in content_dict.values():
        #     self.ingestor.add_data_to_index_cache(
        #         content, FileMetadata(stream_label="content", stream_source="testing")
        #     )
        # self.indexer.generate_index(self.ingestor.cache_directory)

        # # 2 broad phase results, 0 narrow phase results
        # results = search(snort_query, QueryTypeEnum.SURICATA, self.base_temp_dir, data_callback=fetch_from_dict)
        # self.assertDictEqual(results, {})

        # with open(os.path.join(os.path.dirname(__file__), "data/simplebeacon.pcap"), "rb") as pcap_file:
        #     data: bytes = pcap_file.read()
        #     content_dict[sha256(data).hexdigest()] = data
        # self.ingestor.create_cache_dir()
        # for content in content_dict.values():
        #     self.ingestor.add_data_to_index_cache(
        #         content, FileMetadata(stream_label="content", stream_source="testing")
        #     )
        # self.indexer.generate_index(self.ingestor.cache_directory)

        # # 3 broad phase results, 1 narrow phase result
        # results = search(snort_query, QueryTypeEnum.SURICATA, self.base_temp_dir, data_callback=fetch_from_dict)
        # self.assertDictEqual(
        #     results,
        #     {
        #         "999999:SimpleBeacon domain: www.gamble.co": [
        #             os.path.join(
        #                 self.base_temp_dir,
        #                 "content/cache/7f43c5b4e1377a29fb63d6c3fc5e8bdd6574ad1bd4005c62fb910bfa4d063ad1",
        #             )
        #         ]
        #     },
        # )
